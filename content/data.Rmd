---
title: Data
description:
toc: true
featuredVideo:
featuredImage: images/tree.jpeg
draft: false
---

_______
## First Dataset
  Our first dataset, the NaNDA Parks data “Parks by Census Tract, United States, 2018,” came from the following source: https://www.openicpsr.org/openicpsr/project/117921/version/V1/view. 
In order to open the dataset, you must create a free account. After this, both the dataset and an accompanying dictionary describing the variables (NaNDA_Parks_by_Census_Tract_2018_v1-1.pdf) can be downloaded. 
  This data was collected by Phillippa Clarke, Robert Melendez, and Megan Chenoweth from the University of Michigan’s Institute for Social Research. The research was funded by the US Department of Health and Human Services, National Institutes of Health, National Institute on Aging; and the Department of Health and Human Services, Administration for Community Living, National Institute on Disability, Independent Living, and Rehabilitation Research. 
  This dataset was created by the Trust for Public Land to aid researchers in learning about how access to public parks may contribute to physical and mental health. Since access to parks and greenspace has been shown to positively impact physical activity levels, health while aging, and people’s sense of well-being, this data provides researchers with concrete information about park access within each US census tract so they can further examine the impact of greenspace on health.
  This dataset has 11 columns and about 73,000 rows, which represent the census tracts in the US, excluding island territories but including Alaska and Hawai’i. The data is from 2018, but was collected in 2020. It describes the number and area of parks for each census tract. 
  The major variables that we considered were “count_open_parks”, which gave the total number of open parks in the census tract, “tract_area_sqmiles”, which described the total area of the census tract in square miles, and “tot_park_area_sqmiles”, which described the total area of parks within the census tract in square miles. These two variables enabled the calculation of “prop_park_area_tract”, which was the proportion of open park land within the total area of the census tract. We also used “tract_fips10”, the code for the census tract, during our data cleaning in order to isolate specific areas of the US and organize the data by county instead of census tract. This data cleaning process is further described below. 
  Other variables in the dataset included count_open_parks_tc10, count_open_parks_tc5, count_open_parks_tc3, and any_open_parks. These were used by the researchers to control for a few census tracts that contained very high numbers of parks, and therefore were top-coded at 10, 5, 3, and 1 (ie, any value greater than n was replaced by the phrase “n or more” in the dataset). Although we explored these variables, they did not end up being extremely relevant to our final analyses. The dataset also included “tract_area” and “tot_park_area”, which described the areas of the census tracts and park lands in square meters, rather than square miles.

________

## Second Dataset
  Our second dataset, Asthma Hospitalization Rates by County (for California), can be found at the following link: https://catalog.data.gov/dataset/asthma-hospitalization-rates-by-county-16dab. This data is publicly accessible and can be freely downloaded, along with the data dictionary. 
  This data was collected and published by the California Department of Public Health, along with California Breathing, Environmental Health Investigations Branch. This dataset covers the period from 2015 to 2019 and contains various statistics and demographics related to hospitalizations for asthma. 
  Publicly funded hospitals publish annual surveys of some important demographic and health information on a generalized level. This information is collected by the state and distributed to the public; however, in accordance with proper deidentification practices, some of this data has been suppressed if the number of hospitalizations in a certain county is so low that someone could identify the individual person based on the dataset information. 
	This dataset contains “county” and “year” variables to identify the geographic location and time period when the data was collected. There is also an “Age group” variable, which contains the following brackets: all ages, 0-4, 0-17, 5-17, 18-64-, 18+, and 65+. The variable “number of hospitalizations” give the number of hospitalizations for asthma in a named county for a named year - per hospitalization, not per person (as in, one person can count for multiple hospitalizations). The variable “age-adjusted hospitalization visit rate” is a numeric that roughly describes the proportion of hospitalizations for a specific county and age group’s population. The “comment” variable states whether data has been suppressed in a specific line of the dataset and why (generally for de-identification purposes). 
	Finally, the “strata” and “strata name” categories contain general and specific demographic levels by which the data has been stratified. This included categories like age group, child vs. adult, and race/ethnicity for the “strata”, and the individual levels/categories were described with “strata name”. We worked a lot with these variables during data cleaning, as described below. 
____

________
All Information To Be Cut Before Final
## Where to keep data?


Below 50mb: In `dataset` folder

Above 50mb: In `dataset_ignore` folder. This folder will be ignored by `git` so you'll have to manually sync these files across your team.

### Sharing your data

For small datasets (<50mb), you can use the `dataset` folder that is tracked by github. Add the files just like you would any other file.

**Do not** use a folder named `data`.
This folder is reserved by `hugo`.
If you create a folder named `data` this will cause problems.

For larger datasets, you'll need to create a new folder in the project root directory named `dataset-ignore`. This will be ignored by git (based off the `.gitignore` file in the project root directory) which will help you avoid issues with Github's size limits. Your team will have to manually make sure the data files in `dataset-ignore` are synced across team members.

Your [load_and_clean_data.R](/load_and_clean_data.R) file is how you will load and clean your data. Here is a an example of a very simple one.

```{r}
source(
  here::here("static", "load_and_clean_data.R"),
  echo = TRUE # Use echo=FALSE or omit it to avoid code output  
)
```

Note how I use the `here::here` function from the [`here` package](https://here.r-lib.org/articles/here.html) to avoid path problems.
This function is used to specify directories within the project's root directory.
You should never use absolute paths (eg. `/Users/danielsussman/path/to/project/` or `C:\\MA415\\Final_Project\\`) and using `here` ensures that it doesn't matter what the current working directory is as long as you have this RStudio Project open.

----



## Files in static

### `load_and_clean_data.R`

The idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them.
This file might create a derivative data set that you then use for your subsequent analysis.
Note that you don't need to run this script from every post/page.
Instead, you can load in the results of this script, which could be plain text files or `.RData` files. In your data page you'll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes.
To link to this file, you can use `[cleaning script](/load_and_clean_data.R)`. 


When you are loading in data, I recommend using the `here::here` function to specify the file path. This function is used to specify a path relative to your project's root directory. Hence, you can read a file using eg, `read_csv(here::here("dataset/data_file.csv"))`.


### Shiny Interactive

If you are using a shiny interactive, you'll need to keep that in a separate folder (i.e. not in `static` or `content`). For the shiny interactive you'll need to publish the app on `shinyapps.io` where the app can be run. When you publish, make sure to include any data sets you are loading in among the files you publish since those datasets will need to be loaded by your app. 





----

## Rubric: On this page

you will

* Describe where/how to find data.
  * You must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.
  * Why was the data collected/curated? Who put it together? (This is important, if you don't know why it was collected then that might not be a good dataset to look at.
* Describe the different data files used and what each variable means. 
  * If you have many variables then only describe the most relevant ones and summarize the rest.
* Describe any cleaning you had to do for your data.
  * You *must* include a link to your `load_and_clean_data.R` file.
  * Also, describe any additional R packages you used outside of those covered in class.
  * Describe and show code for how you combined multiple data files and any cleaning that was necessary for that.
  * Some repetition of what you do in your `load_and_clean_data.R` file is fine and encouraged if it helps explain what you did.
* Organization, clarity, cleanliness of the page
  * Make sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.